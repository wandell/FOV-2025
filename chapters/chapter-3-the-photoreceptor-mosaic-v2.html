<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>3&nbsp; The Photoreceptor Mosaic – Foundations of Vision (2nd Edition)</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../chapters/chapter-4-wavelength-encoding-v2.html" rel="next">
<link href="../chapters/chapter-2-image-formation-v2.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating slimcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../chapters/part-1-image-encoding-v2.html">Image Encoding</a></li><li class="breadcrumb-item"><a href="../chapters/chapter-3-the-photoreceptor-mosaic-v2.html"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">The Photoreceptor Mosaic</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Foundations of Vision (2nd Edition)</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">How to study vision</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Image Encoding</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/part-1-image-encoding-v2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction to Image Encoding</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/chapter-2-image-formation-v2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Image Formation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/chapter-3-the-photoreceptor-mosaic-v2.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">The Photoreceptor Mosaic</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/chapter-4-wavelength-encoding-v2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Wavelength Encoding</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Image Representation</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/part-2-image-representation-v2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction to Image Representation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/chapter-5-the-retinal-representation-v2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Retina</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/chapter-6-the-cortical-representation-v2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Cortical Representation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/chapter-7-pattern-sensitivity-v2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Pattern vision</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/chapter-8-multiresolution-image-representations-v2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Multiresolution representations</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Image Interpretation</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/part-3-image-interpretation-v2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction to Image Interepretation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/chapter-9-color-v2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Color</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/chapter-10-motion-and-depth-v2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Motion and depth</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/chapter-11-seeing-v2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Seeing</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">Appendix</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/appendix-v2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Appendix</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/appendix-computational-examples-v2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Example Programs</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/appendix-online-teaching-resources-v2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Online Teaching Resources</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#the-photoreceptor-types" id="toc-the-photoreceptor-types" class="nav-link active" data-scroll-target="#the-photoreceptor-types"><span class="header-section-number">3.1</span> The photoreceptor types</a>
  <ul class="collapse">
  <li><a href="#units-of-visual-angle" id="toc-units-of-visual-angle" class="nav-link" data-scroll-target="#units-of-visual-angle"><span class="header-section-number">3.1.1</span> Units of Visual Angle</a></li>
  </ul></li>
  <li><a href="#the-s-cone-mosaic" id="toc-the-s-cone-mosaic" class="nav-link" data-scroll-target="#the-s-cone-mosaic"><span class="header-section-number">3.2</span> The S Cone Mosaic</a>
  <ul class="collapse">
  <li><a href="#behavioral-measurements" id="toc-behavioral-measurements" class="nav-link" data-scroll-target="#behavioral-measurements"><span class="header-section-number">3.2.1</span> Behavioral Measurements</a></li>
  <li><a href="#biological-measurements" id="toc-biological-measurements" class="nav-link" data-scroll-target="#biological-measurements"><span class="header-section-number">3.2.2</span> Biological Measurements</a></li>
  <li><a href="#why-are-the-s-cones-widely-spaced" id="toc-why-are-the-s-cones-widely-spaced" class="nav-link" data-scroll-target="#why-are-the-s-cones-widely-spaced"><span class="header-section-number">3.2.3</span> Why are the S cones widely spaced?</a></li>
  </ul></li>
  <li><a href="#visual-interferometry" id="toc-visual-interferometry" class="nav-link" data-scroll-target="#visual-interferometry"><span class="header-section-number">3.3</span> Visual Interferometry</a></li>
  <li><a href="#sampling-and-aliasing" id="toc-sampling-and-aliasing" class="nav-link" data-scroll-target="#sampling-and-aliasing"><span class="header-section-number">3.4</span> Sampling and Aliasing</a></li>
  <li><a href="#the-l-and-m-cone-mosaic" id="toc-the-l-and-m-cone-mosaic" class="nav-link" data-scroll-target="#the-l-and-m-cone-mosaic"><span class="header-section-number">3.5</span> The L and M Cone Mosaic</a>
  <ul class="collapse">
  <li><a href="#visual-interferometry-measurements-of-human-optics" id="toc-visual-interferometry-measurements-of-human-optics" class="nav-link" data-scroll-target="#visual-interferometry-measurements-of-human-optics"><span class="header-section-number">3.5.1</span> Visual Interferometry: Measurements of Human Optics</a></li>
  </ul></li>
  <li><a href="#summary-and-discussion" id="toc-summary-and-discussion" class="nav-link" data-scroll-target="#summary-and-discussion"><span class="header-section-number">3.6</span> Summary and Discussion</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../chapters/part-1-image-encoding-v2.html">Image Encoding</a></li><li class="breadcrumb-item"><a href="../chapters/chapter-3-the-photoreceptor-mosaic-v2.html"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">The Photoreceptor Mosaic</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span id="sec-photoreceptor-mosaic" class="quarto-section-identifier"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">The Photoreceptor Mosaic</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>In <a href="../chapter-2-image-formation/">Chapter 2</a> we reviewed Campbell and Gubisch’s (1967) measurements of the optical linespread function. Their data are presented in Figure 2.12, as smooth curves, but the actual measurements must have taken place at a series of finely spaced intervals called sample points. In designing their experiment, Campbell and Gubisch must have considered carefully how to space their sample points because they wanted to space their measurement samples only finely enough to capture the intensity variations in the measurement plane. Had they positioned their samples too widely, then they would have missed significant variations in the data. On the other hand, spacing the sample positions too closely would have made the measurement process wasteful of time and resources.</p>
<p>Just as Campbell and Gubisch sampled their linespread measurements, so too the retinal image is sampled by the nervous system. Since only those portions of the retinal image that stimulate the visual photoreceptors can influence vision, the sample positions are determined by the positions of the photoreceptors. If the photoreceptors are spaced too widely, the image encoding will miss significant variation present in the retinal image. On the other hand, if the photoreceptors are spaced very close to one another compared to the spatial variation that is possible given the inevitable optical blurring, then the image encoding will be redundant, using more neurons than necessary to do the job. In this chapter we will consider how the spatial arrangement of the photoreceptors, called the photoreceptor mosaic, limits our ability to infer the spatial pattern of light intensity present in the retinal image.</p>
<section id="the-photoreceptor-types" class="level2 page-columns page-full" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="the-photoreceptor-types"><span class="header-section-number">3.1</span> The photoreceptor types</h2>
<p>We will consider separately the photoreceptor mosaics of each of the different types of photoreceptors. There are two fundamentally different types of photoreceptors in our eye, the rods and the cones. There are approximately 5 million cones and 100 million rods in each eye. The positions of these two types of photoreceptors differ in many ways across the retina. Figure 3.1 shows how the relative densities of cone photoreceptors and rod photoreceptors vary across the retina.</p>
<div id="fig-rod-cone-distribution" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-rod-cone-distribution-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/02/rod.cone_.distribution2-1024x467.png" class="img-fluid figure-img" style="width:70.0%">
</div>
<figcaption class="quarto-float-caption-margin quarto-float-caption quarto-float-fig margin-caption" id="fig-rod-cone-distribution-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.1: The distribution of rod and cone photorceptors across the human retina. (a) The density of the receptors is shown in degrees of visual angle relative to the position of the fovea for the left eye. (b) The cone receptors are concentrated in the fovea. The rod photoreceptors are absent from the fovea and reach their highest density 10 to 20 degrees peripheral to the fovea. No photoreceptors are present in the blindspot.
</figcaption>
</figure>
</div>
<p>The rods initiate vision under low illumination levels, called scotopic light levels, while the cones initiate vision under higher, photopic light levels. The range of intensities in which both rods and cones can initiate vision is called mesopic intensity levels. At most wavelengths of light, the cones are less sensitive to light than the rods. This sensitivity difference, coupled with the fact that there are no rods in the fovea, explains why we can not see very dim sources, such as weak starlight, when we fixate our fovea directly on them. These sources are too dim to be visible through the all cone fovea. The dim source only becomes visible when it is placed in the periphery and be detected by the rods. Rods are very sensitive light detectors: they generate a detectable photocurrent response when they absorb a single photon of light (Hecht et al., 1942; Schwartz, 1978; Baylor et al.&nbsp;1987).</p>
<p>The region of highest visual acuity in the human retina is the <em>fovea</em>. As Figure 3.1 shows, the fovea contains no rods, but it does contain the highest concentration of cones. There are approximately 50,000 cones in the human fovea. Since there are no photoreceptors at the optic disk, where the ganglion cell axons exit the retina, there is a blindspot in that region of the retina (see <a href="../chapter-5-the-retinal-representation/">Chapter 5</a>).</p>
<div id="fig-photoreceptor" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-photoreceptor-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/02/photoreceptor-1024x900.png" class="img-fluid figure-img" width="501">
</div>
<figcaption class="quarto-float-caption-margin quarto-float-caption quarto-float-fig margin-caption" id="fig-photoreceptor-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.2: Mammalian rod and cone photoreceptors contain the light absorbing pigment that initiates vision. Light enters the photoreceptors through the inner segment and is funneled to the outer segment that contains the photopigment. (After Baylor, 1987)
</figcaption>
</figure>
</div>
<p>Figure 3.2 shows schematics of a mammalian rod and a cone photoreceptor. Light imaged by the cornea and lens is shown entering the receptors through the <em>inner segments</em>. The light passes into the <em>outer segment</em> which contain light absorbing <em>photopigments</em>. As light passes from the inner to the outer segment of the photoreceptor, it will either be absorbed by one of the photopigment molecules in the outer segment or it will simply continue through the photoreceptor and exit out the other side. Some light imaged by the optics will pass between the photoreceptors. Overall, less than ten percent of the light entering the eye is absorbed by the photoreceptor photopigments (Baylor, 1987).</p>
<p>The rod photoreceptors contain a photopigment called rhodopsin. The rods are small, there are many of them, and they sample the retinal image very finely. Yet, visual acuity under scotopic viewing conditions is very poor compared to visual acuity under photopic conditions. The reason for this is that the signals from many rods converge onto a single neuron within the retina, so that there is a many-to-one relationship between rod receptors and neurons in the optic nerve fibers. The density of rods and the convergence of their signals onto single neurons improves the sensitivity of rod-initiated vision. Hence, rod-initiated vision does not resolve fine spatial detail.</p>
<p>The foveal cone signals do not converge onto single neurons. Instead, several neurons encode the signal from each cone, so that there is a one-to-many relationship between the foveal cones and optic tract neurons. The dense representation of the foveal cones suggests that the spatial sampling of the cones must be an important aspect of the visual encoding.</p>
<p>There are three types of cone photoreceptors within the human retina. Each cone can be classified based on the wavelength sensitivity of the photopigment in its outer segment. Estimates of the spectral sensitivity of the three types of cone photoreceptors are shown in Figure 3.3. These curves are measured from the cornea, so they include light loss due to the cornea, lens and inert materials of the eye. In the next chapter we will study how color vision depends upon the differences in wavelength selectivity of the three types of cones. Throughout this book I will refer to the three types of photoreceptors as the L, M and S cones.</p>
<p>(The letters refer to <strong>L</strong>ong-wavelength, <strong>M</strong>iddle-wavelength and <strong>S</strong>hort-wavelength peak sensitivity.)</p>
<div id="fig-cone-spectra" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-cone-spectra-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/02/rec.spec_.sens_-1024x791.png" class="img-fluid figure-img" style="width:50.0%">
</div>
<figcaption class="quarto-float-caption-margin quarto-float-caption quarto-float-fig margin-caption" id="fig-cone-spectra-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.3: Spectral sensitivities of the L, M and S cones in the human eye. The measurements are based on a light source at the cornea, so that the wavelength loss due to the cornea, lens and other inert pigments of the eye play a role in determining the sensitivity. (Source: Stockman and Macleod, 1993).
</figcaption>
</figure>
</div>
<div id="fig-cone-mosaic" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-cone-mosaic-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/02/coneMosaic1.png" class="img-fluid figure-img" style="width:70.0%">
</div>
<figcaption class="quarto-float-caption-margin quarto-float-caption quarto-float-fig margin-caption" id="fig-cone-mosaic-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.4: Photoreceptor Sampling: The spatial mosaic of the human cones. A cross-section of the human retina at the level of the inner segments. Cones in the fovea (a) are smaller than cones in the periphery (b). As the separation between cones grows, the rod receptors fill in the spaces. (c) The cone density varies with distance from the fovea. Cone density is plotted as a function of eccentricity for seven human retinae (After Curcio et al, 1990).
</figcaption>
</figure>
</div>
<p>Because light is absorbed after passing through the inner segment, the position of the inner segment determines the spatial sampling position of the photoreceptor. Figure 3.4 shows cross-sections of the human cone photoreceptors at the level of the inner segment in the human fovea (part a) and just outside the fovea (part b). In the fovea, cross-section shows that the inner segments are very tightly packed and form a regular sampling array. A cross-section just outside the fovea shows that the rod photoreceptors fill the spaces between the cones and disrupt the regular packing arrangement. The scale bar represents <span class="math inline">\(10~\mu\text{m}\)</span>; the cone photoreceptor inner segments in the fovea are approximately <span class="math inline">\(2.3~\mu\text{m}\)</span> wide with a minimum center to center spacing of about <span class="math inline">\(2.5~\mu\text{m}\)</span>. Figure 3.4c shows plots of the cone densities from several different human retinae as a function of the distance from the foveal center. The cone density varies across individuals.</p>
<div id="fig-viewing-angle" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-viewing-angle-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/02/viewingangle-1024x590.png" class="img-fluid figure-img" style="width:50.0%">
</div>
<figcaption class="quarto-float-caption-margin quarto-float-caption quarto-float-fig margin-caption" id="fig-viewing-angle-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.5: Calculating Viewing Angle: By trigonometry, the tangent of the viewing angle, \phi, is equal to the ratio of height to distance in the right triangle shown. Therefore, \phi is the inverse tangent of that ratio (Equation e2:viewingAngle).
</figcaption>
</figure>
</div>
<section id="units-of-visual-angle" class="level3" data-number="3.1.1">
<h3 data-number="3.1.1" class="anchored" data-anchor-id="units-of-visual-angle"><span class="header-section-number">3.1.1</span> Units of Visual Angle</h3>
<p>We can convert these cone sizes and separations into degrees of visual angle as follows. The distance from the effective center of of the eye’s optics to the retina is <span class="math inline">\(1.7 \times 10^{-2}~\mathrm{m}\)</span> (17 mm). We compute the visual angle spanned by one cone, <span class="math inline">\(\phi\)</span>, from the trigonometric relationship in <a href="#fig-viewing-angle" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-viewing-angle</span></a>: the tangent of an angle in a right triangle is equal to the ratio of the lengths of the sides opposite and adjacent to the angle. This leads to the following equation:</p>
<p><span id="eq-tanphi"><span class="math display">\[
\tan(\phi) = \frac{2.5 \times 10^{-6}~\mathrm{m}}{1.7 \times 10^{-2}~\mathrm{m}} = 1.47 \times 10^{-4}
\tag{3.1}\]</span></span></p>
<p>The width of a cone in degrees of visual angle, <span class="math inline">\(\phi\)</span>, is approximately <span class="math inline">\(0.0084\)</span> degrees, or roughly one-half minute of visual angle. In the center of the eye, then, where the photoreceptors are packed densely, the cone photoreceptors are tightly packed and their centers are separated by one-half minute of visual angle.</p>
<p><a name="TheSConeMosaic"></a></p>
</section>
</section>
<section id="the-s-cone-mosaic" class="level2 page-columns page-full" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="the-s-cone-mosaic"><span class="header-section-number">3.2</span> The S Cone Mosaic</h2>
<p><a name="BehavioralMeasurements"></a></p>
<section id="behavioral-measurements" class="level3 page-columns page-full" data-number="3.2.1">
<h3 data-number="3.2.1" class="anchored" data-anchor-id="behavioral-measurements"><span class="header-section-number">3.2.1</span> Behavioral Measurements</h3>
<p>Just as the rods and cones have different spatial sampling distributions, so too the three types of cone photoreceptors have different spatial sampling distributions. The sampling distribution of the short-wavelength cones was the first to be measured empirically, and it has been measured both with behavioral and physiological methods. The behavioral experiments were carried out as part of D. Williams dissertation at the University of California in San Diego. Williams, Hayhoe and MacLeod (1981) took advantage of several features of the short-wavelength photoreceptors. As background to their work, we first describe several features of the photoreceptors.</p>
<p>The photopigment in the short-wavelength photoreceptors is significantly different from the photopigment in the other two types of photoreceptors. Notice that the wavelength sensitivity of the L and M photopigments are very nearly the same (Figure 3.3). The sensitivity of the S photopigment is significantly higher in the short-wavelength part of the spectrum than the sensitivity of the other two photopigments. As a result, if we present the visual system with a very weak light, containing energy only in the short-wavelength portion of the spectrum, the S cones will absorb relatively more quanta than the other two classes. Indeed, the discrepancy in the absorptions is so large that it is reasonable to suppose that when short-wavelength light is barely visible, at detection threshold, perception is initiated uniquely from a signal that originates in the short-wavelength receptors.</p>
<p>We can give the short-wavelength receptors an even greater sensitivity advantage by presenting a blue test target on a steady yellow background. As we will discuss in later chapters, steady backgrounds suppress visual sensitivity. By using a yellow background, we can suppress the sensitivity of the <span class="math inline">\(\mathrm{L}\)</span> and <span class="math inline">\(\mathrm{M}\)</span> cones and the rods and yet spare the sensitivity of the <span class="math inline">\(\mathrm{S}\)</span> cones. This improves the relative sensitivity advantage of the short-wavelength receptors in detecting the short-wavelength test light. It is reasonable to suppose that when short-wavelength light is barely visible, at detection threshold, perception is initiated uniquely from a signal that originates in the short-wavelength receptors.</p>
<p>During the experiment, the subjects visually fixated on a small mark. They were then presented with short-wavelength test lights that were likely to be seen with a signal initiated by the <span class="math inline">\(\mathrm{S}\)</span> cones. After the eye was perfectly fixated, the subject pressed a button and initiated a stimulus presentation. The test stimulus was a tiny point of light, presented very briefly (10 ms). The test light was presented at different points in the visual field. If light from the short-wavelength test fell upon a region that contained <span class="math inline">\(\mathrm{S}\)</span> cones, sensitivity should be relatively high. On the other hand, if that region of the retina contained no <span class="math inline">\(\mathrm{S}\)</span> cones, sensitivity should be rather low. Hence, from the spatial pattern of visual sensitivity, Williams, Hayhoe and Macleod inferred the spacing of the <span class="math inline">\(\mathrm{S}\)</span> cones.</p>
<div id="fig-s-cone-mosaic" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-s-cone-mosaic-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/02/williams.dat_6.png" class="img-fluid figure-img" style="width:50.0%">
</div>
<figcaption class="quarto-float-caption-margin quarto-float-caption quarto-float-fig margin-caption" id="fig-s-cone-mosaic-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.6: Short-wavelength Cone Mosaic: Psychophysical estimate of the spatial mosaic of the S cones. The height of the surface represents the observer’s threshold sensitivity to a short wavelength test light presented on a yellow background. The test was presented at a series of locations spanning a grid around the fovea (black dot). The peaks in sensitivity probably correspond to the positions of the S cones. (From Williams, Hayhoe, and Macleod, 1981).
</figcaption>
</figure>
</div>
<p>The sensitivity measurements are shown in Figure 3.6. First, notice that in the very center of the visual field, in the central fovea, there is a large valley of low sensitivity. In this region, there appear to be no short-wavelength cones at all. Second, beginning about half a degree from the center of the visual field there are small, punctate spatial regions of high sensitivity. We interpret these results by assuming that these peaks correspond to the positions of this observer’s <span class="math inline">\(\mathrm{S}\)</span> cones. The gaps in between, where the observer has rather low sensitivity are likely to be patches of <span class="math inline">\(\mathrm{L}\)</span> and <span class="math inline">\(\mathrm{M}\)</span> cones. Around the central fovea, the typical separation between the inferred <span class="math inline">\(\mathrm{S}\)</span> cones is about 8 to 12 minutes of visual angle. Thus, there are five to seven <span class="math inline">\(\mathrm{S}\)</span> cones per degree of visual angle.</p>
<p><a name="BiologicalMeasurements"></a></p>
</section>
<section id="biological-measurements" class="level3" data-number="3.2.2">
<h3 data-number="3.2.2" class="anchored" data-anchor-id="biological-measurements"><span class="header-section-number">3.2.2</span> Biological Measurements</h3>
<p>There have been several biological measurements of the short-wavelength cone mosaic, and we can compare these with the behavioral measurements. Marc and Sperling (1977) used a stain that is taken up by cones when they are active. They applied this stain to a baboon retina and then stimulated the retina with short-wavelength light in the hopes of staining only the short-wavelength receptors. They found that only a few cones were stained when the stimulus was a short-wavelength light. The typical separation between the stained cones was about 6 minutes of arc. This value is smaller than the separation that Williams’ et al.&nbsp;observed and may be a species-related difference.</p>
<p>F. DeMonasterio, S. Schein, and E. McCrane (1981) discovered that when the dye procion yellow is applied to the retina, the dye is absorbed in the outer segments of all the photoreceptors, but it stains only a small subset of the photoreceptors completely. Figure 3.7 shows a group of stained photoreceptors in cross-section.</p>
<p>The indirect arguments identifying these special cones as <span class="math inline">\(\mathrm{S}\)</span> cones are rather compelling. But, a more certain procedure was developed by C. Curcio and her colleagues. They used a biological marker, developed based on knowledge of the genetic code for the <span class="math inline">\(\mathrm{S}\)</span> cone photopigment, to label selectively the <span class="math inline">\(\mathrm{S}\)</span> cones in the human retina (Curcio, et al.&nbsp;1991). Their measurements agree well quantitatively with Williams’ psychophysical measurements, namely that the average spacing between the <span class="math inline">\(\mathrm{S}\)</span> cones is 10 minutes of visual angle. Curcio and her colleagues could also confirm some early anatomical observations that the size and shape of the <span class="math inline">\(\mathrm{S}\)</span> cones differ slightly from the <span class="math inline">\(\mathrm{L}\)</span> and <span class="math inline">\(\mathrm{M}\)</span> cones. The <span class="math inline">\(\mathrm{S}\)</span> cones have a wider inner segment, and they appear to be inserted within an orderly sampling arrangement of their own between the sampling mosaics of the other two cone types (Ahnelt, Kolb and Pflug, 1987).</p>
<p><a name="Whyarethecones"></a></p>
</section>
<section id="why-are-the-s-cones-widely-spaced" class="level3 page-columns page-full" data-number="3.2.3">
<h3 data-number="3.2.3" class="anchored" data-anchor-id="why-are-the-s-cones-widely-spaced"><span class="header-section-number">3.2.3</span> Why are the S cones widely spaced?</h3>
<p>The spacing between the <span class="math inline">\(\mathrm{S}\)</span> cones is much larger than the spacing between the <span class="math inline">\(\mathrm{L}\)</span> and <span class="math inline">\(\mathrm{M}\)</span> cones. Why should this be? The large spacing between the <span class="math inline">\(\mathrm{S}\)</span> cones is consistent with the strong blurring of the short-wavelength component of the image due to the axial chromatic aberration of the lens. Recall that axial chromatic aberration of the lens blurs the short-wavelength portion of the retinal image, the part <span class="math inline">\(\mathrm{S}\)</span> cones are particularly sensitive to, more than the middle- and long-wavelength portion of the image (Figure 2.12). In fact, under normal viewing conditions the retinal image of a fine line at 450 nm falls to one half its peak intensity nearly 10 minutes of visual angle away from the location of its peak intensity. At that wavelength, the retinal image only contains significant contrast at spatial frequency components below 3 cycles per degree of visual angle. The optical defocus force the wavelength components of the retinal image the <span class="math inline">\(\mathrm{S}\)</span> cones encode to vary smoothly across space. Consequently, the <span class="math inline">\(\mathrm{S}\)</span> cones can sample the image only six times per degree and still recover the spatial variation passed by the cornea and lens.</p>
<div id="fig-blue-cone-mosaic" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-blue-cone-mosaic-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/02/blueConeMosaic.png" class="img-fluid figure-img" style="width:50.0%">
</div>
<figcaption class="quarto-float-caption-margin quarto-float-caption quarto-float-fig margin-caption" id="fig-blue-cone-mosaic-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.7: Short-Wavelength Cone Mosaic: Procion Yellow Stains. Biological estimate of the spatial mosaic of the \Blue cones in the macaque retina. A small fraction of the cones absorb the procion yellow stain; these are shown as the dark spots in this image. These cones, thought to be the\Blue cones, are shown in a cross-section through the inner segment layer of the retina. (From DeMonasterio, Schein and McCrane, 1985)
</figcaption>
</figure>
</div>
<p>Interestingly, the <em>spatial</em> defocus of the short-wavelength component of the image also implies that signals initiated by the <span class="math inline">\(\mathrm{S}\)</span> cones will vary slowly over <em>time</em>. In natural scenes, temporal variation occurs mainly because of movement of the observer or an object. When a sharp boundary moves across a cone position, the light intensity changes rapidly at that point. But, if the boundary is blurred, changing gradually over space, then the light intensity changes more slowly. Since the short-wavelength signal is blurred by the optics, and temporal variation is mainly due to motion of objects, the <span class="math inline">\(\mathrm{S}\)</span> cones will generally be coding slower temporal variations than the <span class="math inline">\(\mathrm{L}\)</span> and <span class="math inline">\(\mathrm{M}\)</span> cones.</p>
<p>At the very earliest stages of vision, we see that the properties of different components of the visual pathway fit smoothly together. The optics set an important limit on visual acuity, and the <span class="math inline">\(\mathrm{S}\)</span> cone sampling mosaic can be understood as a consequence of the optical limitations. As we shall see, the <span class="math inline">\(\mathrm{L}\)</span> and <span class="math inline">\(\mathrm{M}\)</span> cone mosaic densities also make sense in terms of the optical quality of the eye.</p>
<p>This explanation of the <span class="math inline">\(\mathrm{S}\)</span> cone mosaic flows from our assumption that visual acuity is the main factor governing the photoreceptor mosaic. For the visual streams initiated by the cones, this is a reasonable assumption. There are other important factors, however, that can play a role in the design of a visual pathway. For example, acuity is not the dominant factor in the visual stream initiated by rod vision. In principle the resolution available in the rod encoding is comparable to the acuity available in the cone responses; but, visual acuity using rod-initiated signals is very poor compared to acuity using cone-initiated signals. Hence, we shouldn’t think of the rod sampling mosaic in terms of visual acuity. Instead, the high density of the rods and their convergence onto individual neurons suggests that we think of the imperative of rod-initiated vision in terms of improving the signal-to-noise under low light levels. In the rod-initiated signals, the visual system trades visual acuity for an increase in the signal-to-noise ratio. In the earliest stages of the visual pathways, then, we can see structure, function and design criteria coming together.</p>
<p>When we ask why the visual system has a particular property, we need to relate observations from the different disciplines that make up vision science. Questions about anatomy require us to think about the behavior the anatomical structure serves. Similarly, behavior must be explained in terms of algorithms and the anatomical and physiological responses of the visual pathway. By considering the visual pathways from multiple points of view, we piece together a complete picture of how system functions.</p>
<p><a name="VisualInterferometry"></a></p>
</section>
</section>
<section id="visual-interferometry" class="level2 page-columns page-full" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="visual-interferometry"><span class="header-section-number">3.3</span> Visual Interferometry</h2>
<div id="fig-young-double-slit" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-young-double-slit-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/02/Young.png" class="img-fluid figure-img" style="width:60.0%">
</div>
<figcaption class="quarto-float-caption-margin quarto-float-caption quarto-float-fig margin-caption" id="fig-young-double-slit-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.8: Young’s double-slit experiment uses a pair of coherent light sources to create an interference pattern of light. The intensity of the resulting image is nearly sinusoidal, and its spatial frequency depends upon the spacing between the two slits.
</figcaption>
</figure>
</div>
<p>Thomas Young (1802), the brilliant scientist, physician, and classicist demonstrated to the Royal Society that when two beams of coherent light generate an image on a surface such as the retinal surface, the resulting image is an interference pattern. His experiment is often called the <em>double-slit</em> or <em>double-pinhole</em> experiment. Using an ordinary light source, Young passed the light through a small pinhole first and then through a pair of slits, as illustrated in Figure 3.8. In the experiment, the first pinhole serves as the source of light; the double pinholes then pass the light from the common original source. Because they share this common source, light emitted from the double pinholes are in a coherent phase relationship and their wavefronts interfere with one another. This interference results in an image that varies nearly sinusoidally in intensity.</p>
<div id="fig-interferometer" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-interferometer-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/02/interferometer.png" class="img-fluid figure-img" style="width:60.0%">
</div>
<figcaption class="quarto-float-caption-margin quarto-float-caption quarto-float-fig margin-caption" id="fig-interferometer-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.9: A visual interferometer creates an interference pattern as in Young’s double-slit experiment. In the device shown here the original beam is split into two paths shown as the solid and dashed lines. (a) When the glass cube is at right angles to the light path, the two beams traverse an equal path and are imaged at the same point after exiting the interferometer. (b) When the glass is rotated, the two beams traverse slightly different paths causing the images of the two coherent beams to be displaced and thus create an interference pattern. (After Macleod, Williams and Makous, 1992).
</figcaption>
</figure>
</div>
<p>We can also achieve this narrow pinhole effect by using a laser as the original source. The key elements of a visual interferometer used by MacLeod et al.&nbsp;(1992) are shown in Figure3.9. Light from a laser enters the beamsplitter and is divided into one part that continues along a straight path (solid line) and a second path that is reflected along a path to the right (dashed line). These two beams, originating from a common source, will be the pair of sources to create the interference pattern on the retina.</p>
<p>Light from each beam is reflected from a mirror towards a glass cube. By varying the orientation of the glass cube, the experimenter can vary the path of the two beams. When the glass cube is at right angles to the light path, as is shown in part (a), the beams continue in a straight path along opposite directions and emerge from the beamsplitter at the same position. When the glass cube is rotated, as is shown in part (b), the refraction due to the glass cube symmetrically changes the beam paths; they emerge from the beamsplitter at slightly different locations and act as a pair of point sources. This configuration creates two coherent beams that act like the two slits in Thomas Young’s experiment, creating an interference pattern. The amount of rotation of the glass cube controls the separation between the two beams.</p>
<p>Each beam passes through only a very small section of the cornea and lens. The usual optical blurring mechanisms do not interfere with the image formation, since the lens does not serve to converge the light (see the section on lenses in <a href="../chapter-2-image-formation/">Chapter 2</a>). Instead, the pattern that is formed depends upon the diffraction due to the restricted spatial region of the light source.</p>
<div id="fig-interference-sinusoid" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-interference-sinusoid-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/02/interference.sinusoid1.png" class="img-fluid figure-img" style="width:60.0%">
</div>
<figcaption class="quarto-float-caption-margin quarto-float-caption quarto-float-fig margin-caption" id="fig-interference-sinusoid-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.10: Sinusoidal Interference Pattern. An interference pattern. The image was created using a double-slit apparatus. The intensity of the pattern is nearly sinusoidal. (From Jenkins and White, 1976.)
</figcaption>
</figure>
</div>
<p>We can use diffraction to create retinal images with much higher spatial frequencies than are possible through ordinary optical imaging by the cornea and lens. Figure 3.10 is an image of a diffraction pattern created by a pair of two slits. The intensity of the pattern is nearly a sinusoidal function of retinal position. The spatial frequency of the retinal image can be controlled by varying the separation between the focal points; the smaller the separation between the slit, the lower the spatial frequency in the interference pattern. Thus, by rotating the glass cube in the interferometer and changing the separation of the two beams we can control the spatial frequency of the retinal image.</p>
<p>Visual interferometry permits us to image fine spatial patterns at much higher contrast than when we image these patterns using ordinary optical methods. For example, Figure 2.14 shows that a <img src="https://foundationsofvision.vista.su.domains/wp-content/ql-cache/quicklatex.com-0cff84480f794b6207081ca077474636_l3.png" title="Rendered by QuickLaTeX.com" class="img-fluid" alt="60"> cycles per degree sinusoid cannot exceed 10 percent contrast when imaged through the optics. Using a visual interferometer, we can present patterns at frequencies considerably higher than <img src="https://foundationsofvision.vista.su.domains/wp-content/ql-cache/quicklatex.com-0cff84480f794b6207081ca077474636_l3.png" title="Rendered by QuickLaTeX.com" class="img-fluid" alt="60"> cycles per degree at 100 percent contrast.</p>
<p>But a challenge remains: the interferometric patterns are not fine lines or points, but rather extended patterns (cosinusoids). Therefore, we cannot use the same logic as Williams et al.&nbsp;and map the receptors by carefully positioning the stimulus. We need to think a little bit more about how to use the cosinusoidal interferometric patterns to infer the structure of the cone mosaic.</p>
<p><a name="SamplingandAliasing"></a></p>
</section>
<section id="sampling-and-aliasing" class="level2 page-columns page-full" data-number="3.4">
<h2 data-number="3.4" class="anchored" data-anchor-id="sampling-and-aliasing"><span class="header-section-number">3.4</span> Sampling and Aliasing</h2>
<div id="fig-aliasing" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-aliasing-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/02/aliasing.png" class="img-fluid figure-img" style="width:60.0%">
</div>
<figcaption class="quarto-float-caption-margin quarto-float-caption quarto-float-fig margin-caption" id="fig-aliasing-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.11: Aliasing of signals results when sampled values are the same but in-between values are not. (a,b) The continuous sinusoids on the left have the same values at the sample positions indicated by the black squares. The values of the two functions at the sample positions are shown by the height of the stylized arrows on the right. (c) Undersampling may cause us to confuse various functions, not just sinusoids. The two curves at the bottom have the same values at the sampled points, differing only in between the sample positions.
</figcaption>
</figure>
</div>
<p>The most basic observation concerning sampling and aliasing is this: we can measure only that portion of the input signal that falls over the sample positions. Figure 3.11 shows one-dimensional examples of aliasing and sampling. Parts (a) and (b) contain two different cosinusoidal signals (left) and the locations of the sample points. The values of these two cosinusoids at the sample points are shown by the height of the arrows on the right. Although the two continuous cosinusoids are quite different, they have the same values at the sample positions. Hence, if cones are only present at the sample positions, the cone responses will not distinguish between these two inputs. We say that these two continuous signals are an <em>aliased</em> pair. Aliased pairs of signals are indistinguishable after sampling. Hence, sampling degrades our ability to discriminate between sinusoidal signals.</p>
<p>Figure ?? shows that sampling degrades our ability to discriminate between signals in general, not just between sinusoids. Whenever two signals agree at the sample points, their sampled representations agree. The basic phenomenon of aliasing is this: Signals that only differ between the sample points are indistinguishable after sampling.</p>
<div id="fig-alias-example" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-alias-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/02/aliasExample.png" class="img-fluid figure-img" style="width:60.0%">
</div>
<figcaption class="quarto-float-caption-margin quarto-float-caption quarto-float-fig margin-caption" id="fig-alias-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.12: Square-wave aliasing. The squarewave on top is seen accurately through the grid. The squarewave on the bottom is at a higher spatial frequency than the grid sampling. When seen through the grid, the pattern appears at a lower spatial frequency and rotated.
</figcaption>
</figure>
</div>
<p>The exercises at the end of this chapter include some computer programs that can help you make sampling demonstrations like the one in Figure 3.12. If you print out squarewave patterns and various sampling arrays, using the programs provided, you can print various patterns onto overhead transparencies and explore the effects of sampling. Figure 3.12 shows an example of two squarewave patterns seen through a sampling grid. After sampling, the high frequency pattern appears to be a rotated, low frequency signal.</p>
<section id="sampling-is-a-linear-operation" class="level4" data-number="3.4.0.1">
<h4 data-number="3.4.0.1" class="anchored" data-anchor-id="sampling-is-a-linear-operation"><span class="header-section-number">3.4.0.1</span> Sampling is a Linear Operation</h4>
<p>The sampling transformation takes the retinal image as input and generates a portion of the retinal image as output. Sampling is a linear operation as the following thought experiment reveals. Suppose we measure the sample values at the cone positions when we present image <span class="math inline">\(A\)</span>; call the intensities at the sample positions <span class="math inline">\(S(A)\)</span>. Now, measure the intensities at the sample positions for a second image, <span class="math inline">\(B\)</span>; call the sample intensities <span class="math inline">\(S(B)\)</span>. If we add together the two images, the new image, <span class="math inline">\(A + B\)</span>, contains the sum of the intensities in the original images. The values picked out by sampling will be the sum of the two sample vectors, <span class="math inline">\(S(A) + S(B)\)</span>.</p>
<p>Since sampling is a linear transformation, we can express it as a matrix multiplication. In our simple description, each position in the retinal image either falls within a cone inner segment or not. The sampling matrix consists of <span class="math inline">\(N\)</span> rows representing the <span class="math inline">\(N\)</span> sampled values. Each row is all zero except at the entry corresponding to that row’s sampling position, where the value is <span class="math inline">\(1\)</span>.</p>
</section>
<section id="aliasing-of-harmonic-functions" class="level4" data-number="3.4.0.2">
<h4 data-number="3.4.0.2" class="anchored" data-anchor-id="aliasing-of-harmonic-functions"><span class="header-section-number">3.4.0.2</span> Aliasing of harmonic functions</h4>
<p>For uniform sampling arrays we have already observed that some pairs of sinusoidal stimuli are aliases of one another (part (a) of Figure 3.11). We can analyze precisely which pairs of sinusoids form alias pairs using a little bit of algebra. Suppose that the continuous input signal is <span class="math inline">\(\cos ( 2 \pi f x )\)</span>. When we sample the stimulus at regular intervals, the output values will be the value of the cosinusoid at those regularly spaced sample points. Suppose that within a single unit of distance there are <span class="math inline">\(N\)</span> sample points, so that our measurements of the stimulus takes place every <span class="math inline">\(1 / N\)</span> units. Then the sampled values will be <span class="math inline">\(S_{f} ( k ) = \cos \left( 2 \pi f \frac{k}{N} \right)\)</span>. A second cosinusoid, at frequency <span class="math inline">\(f'\)</span> will be an alias if its sample values are equal, that is, if <span class="math inline">\(S_{f'} (k) = S_{f} (k)\)</span>.</p>
<p>With a little trigonometry, we can prove that the sample values for any pair of cosinusoids with frequencies <span class="math inline">\(\frac{N}{2} - f\)</span> and <span class="math inline">\(\frac{N}{2} + f\)</span> will be equal. That is,</p>
<p><span id="eq-cos-alias"><span class="math display">\[
\cos\left( 2\pi \left( \frac{N}{2} + f \right) \frac{k}{N} \right) = \cos\left( 2\pi \left( \frac{N}{2} - f \right) \frac{k}{N} \right)
\tag{3.2}\]</span></span></p>
<p>(To prove this we must use the cosine addition law to expand the right sides of the following equation. The steps in the verification are left as an exercise at the end of the chapter.)</p>
<p>The frequency <span class="math inline">\(f = N / 2\)</span> is called the <em>Nyquist frequency</em> of the uniform sampling array; sometimes it is referred to as the <em>folding frequency</em>. Cosinusoidal stimuli whose frequencies differ by equal amounts above and below the Nyquist frequency of a uniform sampling array will have identical sample responses.</p>
</section>
<section id="experimental-implications" class="level4" data-number="3.4.0.3">
<h4 data-number="3.4.0.3" class="anchored" data-anchor-id="experimental-implications"><span class="header-section-number">3.4.0.3</span> Experimental Implications</h4>
<p>The aliasing calculations suggest an experimental method to measure the spacing of the cones in the eye. If the cone spacing is uniform, then pairs of stimuli separated by equal amounts above and below the Nyquist frequency should appear indistinguishable. Specifically, a signal <span class="math inline">\(\cos \left(2 \pi \left( \frac{N}{2} + f \right) \right)\)</span> that is above the Nyquist frequency will appear the same as the signal <span class="math inline">\(\cos \left(2 \pi \left( \frac{N}{2} - f \right) \right)\)</span> that is an equal amount below the Nyquist frequency. Thus, as subjects view interferometric patterns of increasing frequency, as we cross the Nyquist frequency the perceived spatial frequency should begin to decrease even though the physical spatial frequency of the diffraction pattern increases.</p>
<p>Yellott (1982) examined the aliasing prediction in a nice graphical way. He made a sampling grid from Polyak’s (1957) anatomical estimate of the cone positions. He simply poked small holes in the paper at the cone positions in one of Polyak’s anatomical drawings. We can place any image we like, for example patterns of light and dark bars, behind the grid. The bits of the image that we see are only those that would be seen by the visual system. Any pair of images that differ only in the regions between the holes will be an aliased pair. Yellott introduced the method and proper analysis, but he used Polyak’s (1957) data on the outer segment positions rather than on the positions of the inner segments (Miller and Bernard, 1983).</p>
<p>This experiment is relatively straightforward for the <span class="math inline">\(\mathbf{S}\)</span> cones. Since these cones are separated by about <span class="math inline">\(10\)</span> minutes of visual angle, there are about six <span class="math inline">\(\mathbf{S}\)</span> cones per degree of visual angle. Hence, their Nyquist frequency is <span class="math inline">\(3\)</span> cycles per degree of visual angle (cpd). It is possible to correct for chromatic aberration and to present spatial patterns at these low frequencies through the lens. Such experiments confirm the basic predictions that we will see aliased patterns (Williams and Collier, 1983).</p>
<p><a name="TheLandMConeMosaic"></a></p>
</section>
</section>
<section id="the-l-and-m-cone-mosaic" class="level2 page-columns page-full" data-number="3.5">
<h2 data-number="3.5" class="anchored" data-anchor-id="the-l-and-m-cone-mosaic"><span class="header-section-number">3.5</span> The L and M Cone Mosaic</h2>
<p>Experiments using a visual interferometer to image a high frequency pattern at high contrast on the retina are a powerful way to analyze the sampling mosaic of <span class="math inline">\(\mathrm{L}\)</span> and <span class="math inline">\(\mathrm{M}\)</span> cones. But, even before this was technical feat was possible, Helmholtz’ (1896) noticed that extremely fine patterns, looked at without any special apparatus, can appear wavy. He attributed this observation to sampling by the cone mosaic. His perception of a fine pattern and his graphical explanation of the waviness in terms of cone sampling are shown in part (a) of Figure 3.13 (boxed drawings).</p>
<div id="fig-alias-drawings" class="quarto-float quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-alias-drawings-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/02/aliasDrawings-1024x899.png" class="img-fluid figure-img" style="width:60.0%">
</div>
<figcaption class="quarto-float-caption-margin quarto-float-caption quarto-float-fig margin-caption" id="fig-alias-drawings-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.13: Drawings of perceived aliasing patterns by several different observers. Helmholtz’ observed aliasing of fine patterns which he drew in part H1. He offered an explanation of his observations, in terms of cone sampling, in H2. Byram’s (1944) drawings of three interference patterns at 40, 85 and 150 cpd are labeled B1, B2, and B3. Drawings W1,W2 and W3 are by subjects in Williams’ laboratory who drew their impression of aliasing of an 80 cpd and two patterns at 110 cpd
</figcaption>
</figure>
</div>
<p>G. Byram was the first to describe the appearance of high frequency interference gratings (Byram, 1944). His drawings of the appearance of these patterns are shown in part (b) of the figure. The image on the left shows the appearance of a low frequency pattern diffraction pattern. The apparent spatial frequency of this stimulus is faithful to the stimulus. Byram noted that as the spatial frequency increases towards 60 cpd, the pattern still appears to be a set of fine lines, but they are difficult to see (middle drawing). When the pattern significantly exceeds the Nyquist frequency, it becomes visible again but looks like the low-frequency pattern drawn on the right. Further, he reports that the pattern shimmers and is unstable, probably due to the motion of the pattern with respect to the cone mosaic (Helmholtz, Byram and Williams, 1944).</p>
<p>Over the last 10 years D. Williams’ group has replicated and extended these measurements using an improved visual interferometer. Their fundamental observations are consistent with both Helmholtz and Byram’s reports, but greatly extend and quantify the earlier measurements. The two illustrations on the left of part (c) of Figure 3.13 show Williams’ drawing of 80 cpd and 110 cpd sinusoidal gratings created on the retina using a visual interferometer. The third figure shows an artist’s drawing of a 110 cpd grating. The drawing on the left covers a large portion of the visual field, and the appearance of the patterns varies across the visual field. For example, at 80 cpd the observer sees high contrast stripes at some positions, while the field appears uniform in other parts of the field. The appearance varies, but the stimulus itself is quite uniform. The variation in appearance is due to changes in the sampling density of the cone mosaic. Cone sampling density is lower in the periphery than in the central visual field, so aliasing begins at lower spatial frequencies in the periphery than in the central visual field. If we present a stimulus at a high enough spatial frequency we observe aliasing in the central and peripheral visual field, as the drawings of the 110 cpd patterns in Figure 3.13 show.</p>
<p>There are two extensions of these ideas on aliasing you should consider. First, the cone packing in the fovea occurs in two dimensions, of course, so that we must ask what the appearance of the aliasing will be at different orientations of the sinusoidal stimuli. As the images in Figure 3.12 show, the orientation of the low frequency alias does not correspond with the orientation of the input. By trying the demonstration yourself and rotating the sampling grid, you will see that the direction of motion of the alias does not correspond with the motion of the input stimulus<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>. These kinds of aliasing confusions have also been reported using visual interferometry (Coletta and Williams, 1987).</p>
<p>Second, our analysis of foveal sampling has been based on some rather strict assumptions concerning the cone mosaic. We have assumed that the cones are all of the same type, that their spacing is perfectly uniform, and that they have very narrow sampling apertures. The general model presented in this chapter can be adapted if any one of these assumptions fails to hold true. As an exercise for yourself, a new analysis with altered assumptions might change the properties of the sampling matrix.</p>
<p><a name="Visual Interferometry"></a></p>
<section id="visual-interferometry-measurements-of-human-optics" class="level3" data-number="3.5.1">
<h3 data-number="3.5.1" class="anchored" data-anchor-id="visual-interferometry-measurements-of-human-optics"><span class="header-section-number">3.5.1</span> Visual Interferometry: Measurements of Human Optics</h3>
<p>There is one last idea you should take away from this chapter: Using interferometry, we can estimate the quality of the optics of the eye.</p>
<p>Suppose we ask an observer to set the contrast of a sinusoidal grating, imaged using normal incoherent light. The observer’s sensitivity to the target will depend on the contrast reduction at the optics and the observer’s neural sensitivity to the target. Now, suppose that we create the same sinusoidal pattern using an interferometer. The interferometric stimulus bypasses the contrast reduction due to the optics. In this second experiment, then, the observer’s sensitivity is limited only by the observer’s neural sensitivity. Hence, the sensitivity difference between these two experiments is an estimate of the loss due to the optics.</p>
<p>The visual interferometric method of measuring the quality of the optics has been used on several occasions. While the interferometric estimates are similar to estimates using reflections from the eye, they do differ somewhat. The difference is shown in Figure 2.14, which includes the Westheimer’s estimate of the modulation transfer function, created by fitting data from reflections, along with data and a modulation transfer function obtained from interferometric measurements. The current consensus is that the optical modulation transfer function is somewhat closer to the visual interferometric measurements than the reflection measurements. The reasons for the differences are discussed in several papers (e.g.&nbsp;Campbell and Green, 1965; Williams 1985; Williams et al., 1995).</p>
<p><a name="SummaryandDiscussion"></a></p>
</section>
</section>
<section id="summary-and-discussion" class="level2" data-number="3.6">
<h2 data-number="3.6" class="anchored" data-anchor-id="summary-and-discussion"><span class="header-section-number">3.6</span> Summary and Discussion</h2>
<p>The <span class="math inline">\(\mathbf{S}\)</span> cones are present at a much lower sampling density, and they are absent in the very center of the fovea. Because they are sparse, we can measure the <span class="math inline">\(\mathbf{S}\)</span> cone positions behaviorally using small points of light. The behavioral estimates of the <span class="math inline">\(\mathbf{S}\)</span> cones are also consistent with anatomical estimates of the <span class="math inline">\(\mathbf{S}\)</span> cone spacing.</p>
<p>The wide spacing of the <span class="math inline">\(\mathbf{S}\)</span> cones can be understood in terms of the chromatic aberration of the eye. The eye is ordinarily in focus for the middle-wavelength part of the visual spectrum, and there is very little contrast beyond 2-3 cycles per degree in the short-wavelength part of the spectrum. The sparse <span class="math inline">\(\mathbf{S}\)</span> cone spacing is matched to the poor quality of the retinal image in the short-wavelength portion of the spectrum.</p>
<p>The <span class="math inline">\(\mathbf{L}\)</span> and <span class="math inline">\(\mathbf{M}\)</span> cones are tightly packed in the central fovea, forming a triangular grid that efficiently samples the retinal image. Ordinarily, optical defocus protects us from aliasing in the fovea. Once aliasing between two signals occurs, the confusion cannot be undone. The two signals have created precisely the same spatial pattern of photopigment absorptions; hence, no subsequent processing, through cone to cone interactions or later neural interpolation, can undo the confusion. The optical defocus prevents high spatial frequencies that might alias from being imaged on the retina.</p>
<p>By creating stimuli with a visual interferometer, we bypass the optical defocus and image patterns at very high spatial frequencies on the cone mosaic. From the aliasing properties of these patterns, we can deduce some of the properties of the <span class="math inline">\(\mathbf{L}\)</span> and <span class="math inline">\(\mathbf{M}\)</span> cone mosaics. The aliasing demonstrations show that the foveal sampling grid is regular and contains approximately 120 cones per degree of visual angle. These measurements, in the living human eye, are consistent with the anatomical images obtained of the human eye reported by Curcio and her colleagues (Curcio, et al., 1991).</p>
<p>The precise arrangement of <span class="math inline">\(\mathbf{L}\)</span> and <span class="math inline">\(\mathbf{M}\)</span> cones within the human retina is unknown, though data on this point should arrive shortly (e.g., Bowmaker and Mollon, 1993). Current behavioral estimates of the relative number of <span class="math inline">\(\mathbf{L}\)</span> and <span class="math inline">\(\mathbf{M}\)</span> cones suggest that there are about twice as many <span class="math inline">\(\mathbf{L}\)</span> cones as <span class="math inline">\(\mathbf{M}\)</span> cones (Cicerone and Nerger, 1989).</p>
<p>The cone sampling grid becomes more coarse and irregular outside the fovea where rods and other cells enter the spaces between the cones. In these portions of the retina, high frequency patterns presented through interferometry no longer appear as regular low frequency frequency patterns. Rather, because of the disarray in the cone spacing, the high frequency patterns appear to be mottled noise. In the periphery, the cone spacing falls off rapidly enough so that it should be possible to observe aliasing without the use of an interferometer (Yellott, 1982).</p>
<p>In analyzing photoreceptor sampling, we have ignored eye movements. In principle, the variation in receptor intensities during these small eye movements can provide information to permit us to discriminate between the alias pairs. (You can check this effect by studying the images you observe when you experiment with the sampling grids.) The effects of eye movements are often minimized in experiments by flashing the targets briefly. But, even when one examines the interferometric pattern for substantial amounts of time, the aliasing persists. The information available from small eye movements could be very useful; but, the analysis assuming a static eye offers a good account of current empirical measurements, This suggests that the nervous system does not integrate information across minute eye movements to improve visual resolution (Packer and Williams, 1992).</p>



</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p>Use the Postscript program in the appendix section to print out a grid and a fine pattern and try this experiment.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../chapters/chapter-2-image-formation-v2.html" class="pagination-link" aria-label="Image Formation">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Image Formation</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../chapters/chapter-4-wavelength-encoding-v2.html" class="pagination-link" aria-label="Wavelength Encoding">
        <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Wavelength Encoding</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>